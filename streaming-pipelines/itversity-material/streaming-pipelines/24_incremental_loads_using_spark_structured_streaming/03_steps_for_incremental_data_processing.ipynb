{"cells":[{"cell_type":"markdown","source":["## Steps for Incremental Data Processing\n\nLet us get into the steps to read JSON data from the underlying file system and then write to `parquet` file format incrementally using Spark Structured Streaming. Here is what we are going to do for the same.\n\n* Ensure the cluster is configured with instance profile to have access on relevant s3 Buckets (on AWS).\n* Get the GHArchive files which are in compressed JSON format from the source to s3. We will be using `requests` module as data is available using REST APIs.\n* Read the JSON files using `readStream` by inferring the schema. It will create a Dataframe object with `isStreaming` set to true.\n* Write the Dataframe to Parquet file format using `trigger(once=True)`\n* Validate whether the data is populated in parquet file format or not.\n* Add new JSON files in the source location.\n* Run the process once again and validate.\n\n> By the end of it we will be able to convert the JSON files to Parquet file format incrementally. In contrast to traditional pipelines, the checkpoint will be managed by Spark Structured Streaming framework. In traditional batch pipelines, we need to take care of this checkpoint (also known as bookmark or marker)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"887094df-31f0-4f8e-8f0b-6a94fff72b0e"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cc68c2f-9d31-4182-857a-fc07e98a4381"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03 Steps for Incremental Data Processing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2400095431327527}},"nbformat":4,"nbformat_minor":0}